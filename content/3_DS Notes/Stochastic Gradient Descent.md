#ml

[[Gradient Descent]]

Why do we use [[Stochastic Gradient Descent]]?;; To find the derivative of discrete data so we can determine a straight line with the Least Square Error (LSE).

Uses the difference quotient.

The step size is important between derivatives (small then slow) (if large then might miss minimum).

With Stochastic method we can don't need to the entire data set again, we can just add the new information to get improvement.



Gradient descent uses the entire data set.
Stochastic uses random entries to get derivative.
Stochastic Mini-batched descent is the fastest way (groups then does randomly).

